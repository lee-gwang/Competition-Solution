{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon  5 회 생체 광학 데이터 분석 AI 모델링 경진대회\n",
    "## 초보\n",
    "## 2020년 7월 03일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 광한 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "import joblib\n",
    "from tqdm import trange, tqdm, tqdm_notebook\n",
    "\n",
    "# DATA SPLIT\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# EVALUATE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MODEL\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ELSE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_col=train.columns[train.columns.str.contains('src')]\n",
    "dst_col = train.columns[train.columns.str.contains('dst')]\n",
    "\n",
    "def simple_fe(train):\n",
    "    \"\"\"\n",
    "    전체 파장에 대한 src, dst 평균 / mean 투과도 / std 투과도\n",
    "    \"\"\"\n",
    "    train['src_mean']=train[src_col].mean(1)\n",
    "    train['dst_mean']=train[dst_col].mean(1)\n",
    "    train['Trans_mean'] = train['dst_mean']/train['src_mean']\n",
    "\n",
    "    train['src_std']=train[src_col].std(1)\n",
    "    train['dst_std']=train[dst_col].std(1)\n",
    "    train['Trans_std'] = train['dst_std']/train['src_std']\n",
    "\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = simple_fe(train)\n",
    "test = simple_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def binning_fe(train, src_col, dst_col, size=50):\n",
    "    \"\"\"\n",
    "    size 별로 binning후 피쳐 생성\n",
    "    \"\"\"\n",
    "    for i in range(650, 1000-size, size):\n",
    "        temp1=train.loc[:, '%s_src'%i : '%s_src'%(i+size)].mean(1)\n",
    "        temp2=train.loc[:, '%s_dst'%i : '%s_dst'%(i+size)].mean(1)\n",
    "        train['Trans_%s_to_%s_mean'%(i,i+size)] = temp2/temp1\n",
    "        train['Concen_%s_to_%s_mean'%(i,i+size)] = np.log(temp2/temp1) / train['rho']\n",
    "    \n",
    "    return train\n",
    "\n",
    "for i in [10, 20,30,40, 50,100]:\n",
    "    train = binning_fe(train, src_col, dst_col, size=i)\n",
    "    test = binning_fe(test, src_col, dst_col, size=i)\n",
    "############################################################\n",
    "\n",
    "def rolling_fe(train):\n",
    "    \"\"\"\n",
    "    window size별 롤링후 피쳐 생성\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for k in [3, 5]:# window size\n",
    "        temp1 = train[src_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        temp2 = train[dst_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        for i in temp:\n",
    "            train['%s_rolling_size_%s'%(i, k)] = np.log(temp2['%s_dst'%i]/temp1['%s_src'%i])/train['rho']\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = rolling_fe(train)\n",
    "test = rolling_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def rolling_fe2(train):\n",
    "    \"\"\"\n",
    "    3, 5 각 윈도우 사이즈로 만든 피쳐끼리 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for i in temp:\n",
    "        train['%s_rolling_size_3/5'%i] = train['%s_rolling_size_3'%i]/ train['%s_rolling_size_5'%i]\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = rolling_fe2(train)\n",
    "test = rolling_fe2(test)\n",
    "\n",
    "############################################################\n",
    "def rolling_fe3(train, near):\n",
    "    \"\"\"\n",
    "    서로 가까운 영역의 롤링 피쳐들끼리 나눠주기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000-near, 10)]\n",
    "    for i in temp:\n",
    "        train['near_%s_%s_size_3'%(i,i+near)] = train['%s_rolling_size_%s'%(i, 3)] / train['%s_rolling_size_%s'%(i+near, 3)]\n",
    "        train['near_%s_%s_size_5'%(i,i+near)] = train['%s_rolling_size_%s'%(i, 5)] / train['%s_rolling_size_%s'%(i+near, 5)]\n",
    "    return train\n",
    "\n",
    "for i in range(10, 30, 10):\n",
    "    train = rolling_fe3(train,i)\n",
    "    test = rolling_fe3(test, i)\n",
    "############################################################\n",
    "\n",
    "def core_fe(train):\n",
    "    \"\"\"\n",
    "    파장대별로 투과도 피쳐와, 농도와 직결되는 피쳐 생성\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for i in temp:\n",
    "        train['Trans_%s'%i] = train['%s_dst'%i]/train['%s_src'%i]\n",
    "        train['Concen_%s'%i] = np.log(train['%s_dst'%i]/train['%s_src'%i])/train['rho']\n",
    "    return train\n",
    "\n",
    "train = core_fe(train)\n",
    "test = core_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def core_fe2(train):\n",
    "    \"\"\"\n",
    "    모든 파장대의 피쳐끼리 서로 빼기 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('Concen_%s'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_div'%(i,j)] = train[i]/train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = core_fe2(train)\n",
    "test = core_fe2(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def core_fe3(train):\n",
    "    \"\"\"\n",
    "    모든 파장대의 피쳐끼리 서로 빼기 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('Concen_%s'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_sub'%(i,j)] = train[i]-train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = core_fe3(train)\n",
    "test = core_fe3(test)\n",
    "############################################################\n",
    "def core_fe4(train, size=10):\n",
    "    temp_col=[]\n",
    "    for i in [x for x in range(650, 1000-size, size)]:\n",
    "        temp_col.append('Concen_%s_to_%s_mean'%(i,i+size))\n",
    "        \n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['temp_%s_%s'%(i,j)] = train[i]/train[j]\n",
    "                train['temp2_%s_%s'%(i,j)] = train[i]-train[j]\n",
    "    return train\n",
    "\n",
    "for i in [10,20,30,40,50]:\n",
    "    train = core_fe4(train, size=i)\n",
    "    test = core_fe4(test, size=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_features=['id','hhb', 'hbo2', 'ca', 'na']\n",
    "col = [x for x in train.columns if x not in excluded_features]\n",
    "\n",
    "x_train = train[col]\n",
    "y_train = train.loc[:, 'hhb':'na']\n",
    "test = test[col]\n",
    "\n",
    "x_train=x_train.replace([np.inf, -np.inf], np.nan)\n",
    "test=test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "X_train = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_permutation_badfeatures(X_train, y_train, col):\n",
    "    y_train_temp = y_train[col].copy()\n",
    "    threshold = [0.0001]\n",
    "    bad_features1= []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "    def score(X, y):\n",
    "        y_pred = reg.predict(X)\n",
    "        return abs(y-y_pred).mean() \n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg= lgb.LGBMRegressor(boosting_type ='gbdt',n_estimators=20000,num_leaves=32, max_depth=-1, min_child_weight=5, \n",
    "                                 subsample=0.7, colsample_bytree =1, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                            random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "        reg.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                  early_stopping_rounds=50 ,verbose=-1, eval_metric='mae')\n",
    "        \n",
    "        \n",
    "        \n",
    "        base_score, score_decreases = get_score_importances(score,np.array(val_x), np.array(val_y), n_iter=1)\n",
    "        \n",
    "        bad_features1.extend(list(val_x.columns[score_decreases[0] > -threshold[0]]))\n",
    "                             \n",
    "    return bad_features1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y_train.columns:\n",
    "    bad_features = my_permutation_badfeatures(X_train, y_train, col)\n",
    "    pd.DataFrame(bad_features)[0].value_counts().to_csv(\"./bad_features/%s_bad_features.csv\"%col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutation 후 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_train_model(x_train, y_train, x_test, label):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "    pred = np.zeros(x_test.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        if os.path.isfile('./models/%s_%sfold_0.78230.pkl'%(label, n_fold)): # 모델 존재할경우\n",
    "            print('%s model load..'%label)\n",
    "            model= joblib.load('./models/%s_%sfold_0.78230.pkl'%(label,n_fold))\n",
    "        \n",
    "        else: # 모델 없을때는 train\n",
    "            print('train %s '%label)\n",
    "            model= lgb.LGBMRegressor(boosting_type ='dart',n_estimators=50000,num_leaves=64, max_depth=-1, min_child_weight=5, \n",
    "                                     subsample=0.7, colsample_bytree = 0.2, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                                random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "            model.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                      early_stopping_rounds=50 ,verbose=50000, eval_metric='mae')\n",
    "        \n",
    "            models.append(model)\n",
    "        \n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "    \n",
    "        # PREDS\n",
    "        pred += model.predict(x_test)/5.0\n",
    "        \n",
    "    return models, oof, pred\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bad_features는 ~~ github주소\n",
    "permute_models = {}\n",
    "permute_oofs=[]\n",
    "permute_preds=[]\n",
    "for label in y_train.columns:\n",
    "    pred = np.zeros(test.shape[0])\n",
    "    \n",
    "    permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "    bad_features = permute_features[permute_features['count'] >1]['feature'].values \n",
    "    \n",
    "    x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "    x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "\n",
    "    ms, oof, pred =permute_train_model(x_reduced_trn, y_train[label], x_reduced_test, label)\n",
    "\n",
    "    permute_models[label] = ms\n",
    "    permute_oofs.append(oof)\n",
    "    permute_preds.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "\n",
    "a=[]\n",
    "for label,i in zip(labels,permute_oofs):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# permute 용도\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['hhb'] = permute_preds[0]\n",
    "submission['hbo2'] = permute_preds[1]\n",
    "submission['ca'] = permute_preds[2]\n",
    "submission['na'] = permute_preds[3]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/sub_0.78230.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(permute_models, cv):\n",
    "    for label in permute_models:\n",
    "        for n, i in enumerate(permute_models[label]):\n",
    "            joblib.dump(i, './models/%s_%sfold_%s.pkl'%(label,n,cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(permute_models, '0.78230')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prob값을 feature로 사용하여 모델 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(x_train, y_train,label):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "\n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "        model= joblib.load('./models/%s_%sfold_0.78230.pkl'%(label,n_fold))\n",
    "        \n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "\n",
    "    \n",
    "        \n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "threshold = [0.001, 0.0005, 0.0001]\n",
    "oofs=[]\n",
    "for label in labels:\n",
    "    permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "    bad_features = permute_features[permute_features['count'] >1]['feature'].values     \n",
    "    \n",
    "    x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "    x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "    y_train2 = y_train[label].copy()\n",
    "    \n",
    "    oof=predict_model(x_reduced_trn, y_train2, label)\n",
    "    oofs.append(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_features(x_train, oofs, data='train'):\n",
    "    \"\"\"\n",
    "    proba features 추가\n",
    "    \"\"\"\n",
    "    if data =='train':\n",
    "        preds_hhb = oofs[0]\n",
    "        preds_hbo2 = oofs[1]\n",
    "        preds_ca = oofs[2]\n",
    "        preds_na = oofs[3]\n",
    "    elif data =='test':\n",
    "        preds_hhb = oofs['hhb']\n",
    "        preds_hbo2 = oofs['hbo2']\n",
    "        preds_ca = oofs['ca']\n",
    "        preds_na = oofs['na']        \n",
    "\n",
    "    x_train['hhb_prob'] = preds_hhb\n",
    "    x_train['hbo2_prob'] = preds_hbo2\n",
    "    x_train['ca_prob'] = preds_ca\n",
    "    x_train['na_prob'] = preds_na\n",
    "\n",
    "    x_train['hhb/hbo2'] = preds_hhb/preds_hbo2\n",
    "    x_train['hhb/ca'] = preds_hhb/preds_ca\n",
    "    x_train['hhb/na'] = preds_hhb/preds_na\n",
    "    x_train['hbo2/ca'] = preds_hbo2/preds_ca\n",
    "    x_train['hbo2/na'] = preds_hbo2/preds_na\n",
    "    x_train['ca/na'] = preds_ca/preds_na\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "x_train = prob_features(x_train, oofs, data='train')\n",
    "\n",
    "# test\n",
    "stack_sub = pd.read_csv('./submission/sub_0.78230.csv')\n",
    "test = prob_features(test, stack_sub, data='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(x_train, y_train, x_test, seed, label, cv):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "    pred = np.zeros(x_test.shape[0])\n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        if os.path.isfile('./models/%s_%sfold_%s_seed%s.pkl'%(label, n_fold, cv, seed)): # 모델 존재할경우\n",
    "            print('%s model load..'%label)\n",
    "            model= joblib.load('./models/%s_%sfold_%s_seed%s.pkl'%(label,n_fold, cv, seed))\n",
    "        \n",
    "        else: # 모델 없을때는 train\n",
    "            print('train %s '%label)\n",
    "            model= lgb.LGBMRegressor(boosting_type ='dart', n_estimators=20000,num_leaves=64, max_depth=-1, \n",
    "                                     min_child_weight=5, subsample=0.7, colsample_bytree = 0.2, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                                random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "            model.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                      early_stopping_rounds=50 ,verbose=20000, eval_metric='mae')\n",
    "        \n",
    "            models.append(model)\n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "        \n",
    "        # PREDS\n",
    "        pred += model.predict(x_test)/5.0\n",
    "    \n",
    "        \n",
    "    return models, oof, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(seed, cv):\n",
    "    models_re = {}\n",
    "    oofs_re=[]\n",
    "    preds_re=[]\n",
    "    threshold = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "    for label in ['hhb', 'hbo2', 'ca', 'na']:\n",
    "        print('train column : ', label)\n",
    "        pred = np.zeros(test.shape[0])\n",
    "        permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "        bad_features = permute_features[permute_features['count'] >1]['feature'].values     \n",
    "    \n",
    "        x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "        x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "\n",
    "        print(x_reduced_trn.shape[1])\n",
    "        ms, oof, pred =retrain_model(x_reduced_trn, y_train[label], x_reduced_test, seed, label, cv)\n",
    "\n",
    "\n",
    "        models_re[label] = ms\n",
    "        oofs_re.append(oof)\n",
    "        preds_re.append(pred)\n",
    "    \n",
    "    return models_re, oofs_re, preds_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=[42,92,2020] ; cv = [0.79025, 0.77943, 0.78182]\n",
    "models_re1, oofs_re1, preds_re1 = train_model(seed[0], cv[0])\n",
    "models_re2, oofs_re2, preds_re2 = train_model(seed[1], cv[1])\n",
    "models_re3, oofs_re3, preds_re3 = train_model(seed[2], cv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "a=[]\n",
    "for label,i in zip(labels,oofs_re1):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('42 seed oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs_re2):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('92 seed oof mae %.5f'% np.mean(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs_re3):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('2020 seed oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 oof 성능\n",
    "final_oof=[]\n",
    "for i in range(4):\n",
    "    temp_oof=np.zeros(train.shape[0])\n",
    "    temp_oof+= (oofs_re1[i]+oofs_re2[i]+oofs_re3[i])/3.0\n",
    "    final_oof.append(temp_oof)\n",
    "#  \n",
    "a=[]\n",
    "for label,i in zip(labels,final_oof):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission_gwang = pd.read_csv('./sample_submission.csv')\n",
    "submission_gwang['hhb'] = (preds_re1[0] +preds_re2[0] +preds_re3[0]) /3\n",
    "submission_gwang['hbo2'] = (preds_re1[1] +preds_re2[1] +preds_re3[1]) /3\n",
    "submission_gwang['ca'] = (preds_re1[2] +preds_re2[2] +preds_re3[2]) /3\n",
    "submission_gwang['na'] = (preds_re1[3] +preds_re2[3] +preds_re3[3]) /3\n",
    "\n",
    "submission_gwang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_gwang.to_csv('./submission/0.77871_gwang_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"모델 저장\"\"\"\n",
    "\n",
    "def save_model(permute_models, cv, seed=42):\n",
    "    for label in permute_models:\n",
    "        for n, i in enumerate(permute_models[label]):\n",
    "            joblib.dump(i, './models/%s_%sfold_%s_seed%s.pkl'%(label,n, cv, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(models_re1, '0.79025', seed=42)\n",
    "save_model(models_re2, '0.77943', seed=92)\n",
    "save_model(models_re3, '0.78182', seed=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기찬 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "import joblib\n",
    "from tqdm import trange, tqdm, tqdm_notebook\n",
    "\n",
    "# DATA SPLIT\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# EVALUATE\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MODEL\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "# ELSE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"./test.csv\", index_col=0)\n",
    "\n",
    "df = train.append(test)\n",
    "df = df[train.columns]\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_col=train.columns[train.columns.str.contains('src')]\n",
    "dst_col = train.columns[train.columns.str.contains('dst')]\n",
    "\n",
    "def fe3(train):\n",
    "    train['src_mean']=train[src_col].mean(1)\n",
    "    train['dst_mean']=train[dst_col].mean(1)\n",
    "    train['div_feature_src2'] = train['dst_mean']/train['src_mean']\n",
    "\n",
    "    train['src_std']=train[src_col].std(1)\n",
    "    train['dst_std']=train[dst_col].std(1)\n",
    "    train['div_feature_dst2'] = train['dst_std']/train['src_std']\n",
    "\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = fe3(train)\n",
    "test = fe3(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def fe_temp(train, src_col, dst_col, size=50):\n",
    "    for i in range(650, 1000-size, size):\n",
    "        temp1=train.loc[:, '%s_src'%i : '%s_src'%(i+size)].mean(1)\n",
    "        temp2=train.loc[:, '%s_dst'%i : '%s_dst'%(i+size)].mean(1)\n",
    "        train['%s_to_%s_mean_src_dst_div'%(i,i+size)] = temp2/temp1\n",
    "        train['%s_to_%s_mean_src_dst_div_rho'%(i,i+size)] = np.log(temp2/temp1) / train['rho']\n",
    "    \n",
    "    return train\n",
    "\n",
    "for i in [10,30,50,100]:\n",
    "    train = fe_temp(train, src_col, dst_col, size=i)\n",
    "    test = fe_temp(test, src_col, dst_col, size=i)\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def window_rollinng_first(train):\n",
    "    for k in [3, 5]:#window size\n",
    "        temp1 = train[src_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        temp2 = train[dst_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        for i in temp:\n",
    "            train['%s_rolling_win_%s'%(i, k)] = np.log(temp2['%s_dst'%i]/temp1['%s_src'%i])/train['rho']\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = window_rollinng_first(train)\n",
    "test = window_rollinng_first(test)\n",
    "\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def window_rolling_div(train):\n",
    "    for i in temp:\n",
    "        train['%s_rolling_win3/win5'%i] = train['%s_rolling_win_3'%i]/ train['%s_rolling_win_5'%i]\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = window_rolling_div(train)\n",
    "test = window_rolling_div(test)\n",
    "\n",
    "############################################################\n",
    "def window_rolling_div_near(train, near):\n",
    "    temp = [x for x in range(650,1000-near, 10)]\n",
    "    for i in temp:\n",
    "        train['near_%s_%s_win_3'%(i,i+near)] = train['%s_rolling_win_%s'%(i, 3)] / train['%s_rolling_win_%s'%(i+near, 3)]\n",
    "        train['near_%s_%s_win_5'%(i,i+near)] = train['%s_rolling_win_%s'%(i, 5)] / train['%s_rolling_win_%s'%(i+near, 5)]\n",
    "    return train\n",
    "\n",
    "for i in range(10, 30, 10):\n",
    "    train = window_rolling_div_near(train,i)\n",
    "    test = window_rolling_div_near(test, i)\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def temp_fe(train):\n",
    "    for i in temp:\n",
    "        train['%s_dd'%i] = train['%s_dst'%i]/train['%s_src'%i]\n",
    "        train['%s_dd2'%i] = np.log(train['%s_dst'%i]/train['%s_src'%i])/train['rho']\n",
    "    return train\n",
    "\n",
    "train = temp_fe(train)\n",
    "test = temp_fe(test)\n",
    "\n",
    "############################################################\n",
    "def near_all(train):\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('%s_dd2'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_dd2'%(i,j)] = train[i]/train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = near_all(train)\n",
    "test = near_all(test)\n",
    "\n",
    "############################################################\n",
    "def near_all(train):\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('%s_dd2'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_dd3'%(i,j)] = train[i]-train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "#train = near_all(train)\n",
    "#test = near_all(test)\n",
    "############################################################\n",
    "\n",
    "def exp1(train, size=10):\n",
    "    temp_col=[]\n",
    "    for i in [x for x in range(650, 1000-size, size)]:\n",
    "        temp_col.append('%s_to_%s_mean_src_dst_div_rho'%(i,i+size))\n",
    "        \n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['fucking_%s_%s'%(i,j)] = train[i]/train[j]\n",
    "                #train['fucking2_%s_%s'%(i,j)] = train[i]-train[j]\n",
    "    return train\n",
    "\n",
    "train = exp1(train, size=10)\n",
    "test = exp1(test, size=10)\n",
    "\n",
    "train = exp1(train, size=30)\n",
    "test = exp1(test, size=30)\n",
    "train = exp1(train, size=50)\n",
    "test = exp1(test, size=50)\n",
    "train = exp1(train, size=100)\n",
    "test = exp1(test, size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace([np.inf, -np.inf], np.nan)\n",
    "train=train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df1 = train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Initial Modeling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"./test.csv\", index_col=0)\n",
    "\n",
    "df = train.append(test)\n",
    "df = df[train.columns]\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = ['dst','src']\n",
    "X_dst = df.iloc[:,np.where(df.columns.str.find(kind[0]) == 4)[0]].copy()\n",
    "X_src = df.iloc[:,np.where(df.columns.str.find(kind[1]) == 4)[0]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absor_rho(df, X_dst, X_src, window):\n",
    "\n",
    "    temp1 = X_dst.copy()\n",
    "    temp2 = X_src.copy()\n",
    "\n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    for i in range(0, len(X_dst)):\n",
    "        temp1.iloc[i,:] = np.log(np.array(temp1.iloc[i,:].rolling(window=window,min_periods=1).mean())/\\\n",
    "                                 np.array(temp2.iloc[i,:].rolling(window=window,min_periods=1).mean()))/df.loc[i,'rho']\n",
    "        \n",
    "    absor_rolling = temp1.copy()\n",
    "    absor_rolling.columns =  X_dst.columns + \"/\" + X_src.columns + \"_rho_\" + str(window)\n",
    "    \n",
    "    \n",
    "    absor_rolling = absor_rolling.replace(np.inf, np.nan)\n",
    "    absor_rolling = absor_rolling.replace(-np.inf, np.nan)\n",
    "    \n",
    "    absor_rolling[\"rolling_mean_\" + str(window)] = absor_rolling.mean(axis=1)\n",
    "    absor_rolling[\"rolling_std_\" + str(window)] = absor_rolling.std(axis=1)   \n",
    "    \n",
    "    \n",
    "    return absor_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absor(df, X_dst, X_src, window):\n",
    "\n",
    "    temp1 = X_dst.copy()\n",
    "    temp2 = X_src.copy()\n",
    "\n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    for i in range(0, len(X_dst)):\n",
    "        temp1.iloc[i,:] = np.array(temp1.iloc[i,:].rolling(window=window,min_periods=1).mean())/\\\n",
    "                                 np.array(temp2.iloc[i,:].rolling(window=window,min_periods=1).mean())\n",
    "        \n",
    "    absor_rolling = temp1.copy()\n",
    "    absor_rolling.columns =  X_dst.columns + \"/\" + X_src.columns + \"_\" + str(window)\n",
    "    \n",
    "    \n",
    "    absor_rolling = absor_rolling.replace(np.inf, np.nan)\n",
    "    absor_rolling = absor_rolling.replace(-np.inf, np.nan)\n",
    "    \n",
    "    absor_rolling[\"rolling_mean_\" + str(window)] = absor_rolling.mean(axis=1)\n",
    "    absor_rolling[\"rolling_std_\" + str(window)] = absor_rolling.std(axis=1)   \n",
    "    \n",
    "    \n",
    "    return absor_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_each_col(df, stride):\n",
    "    temp = df.copy()\n",
    "    for i in range(stride, len(temp.columns)):\n",
    "        temp[temp.columns[i-stride] + \"/\" + temp.columns[i]] = temp[temp.columns[i-stride]]/temp[temp.columns[i]]\n",
    "        \n",
    "    return temp.iloc[:,len(df.columns):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_each_col(df, stride):\n",
    "    temp = df.copy()\n",
    "    for i in range(stride, len(temp.columns)):\n",
    "        temp[temp.columns[i-stride] + \"-\" + temp.columns[i]] = temp[temp.columns[i-stride]] - temp[temp.columns[i]]\n",
    "        \n",
    "    return temp.iloc[:,len(df.columns):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_df(df1, df2):\n",
    "    temp1 = df1.copy()\n",
    "    temp2 = df2.copy()\n",
    "    \n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    df = temp1/temp2\n",
    "    df.columns = df1.columns + \"/\" + df2.columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rolling1 = get_absor(df, X_dst, X_src, 1)\n",
    "absor_rolling2 = get_absor(df, X_dst, X_src, 3)\n",
    "absor_rolling3 = get_absor(df, X_dst, X_src, 5)\n",
    "absor_rolling4 = get_absor(df, X_dst, X_src, 10)\n",
    "\n",
    "absor_rho_rolling1 = get_absor_rho(df, X_dst, X_src, 1)\n",
    "absor_rho_rolling2 = get_absor_rho(df, X_dst, X_src, 3)\n",
    "absor_rho_rolling3 = get_absor_rho(df, X_dst, X_src, 5)\n",
    "absor_rho_rolling4 = get_absor_rho(df, X_dst, X_src, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rolling1_divide_stride1 = divide_each_col(absor_rolling1, 1)\n",
    "absor_rolling1_divide_stride2 = divide_each_col(absor_rolling1, 3)\n",
    "absor_rolling1_divide_stride3 = divide_each_col(absor_rolling1, 5)\n",
    "absor_rolling1_divide_stride4 = divide_each_col(absor_rolling1, 10)\n",
    "\n",
    "absor_rolling2_divide_stride1 = divide_each_col(absor_rolling2, 1)\n",
    "absor_rolling2_divide_stride2 = divide_each_col(absor_rolling2, 3)\n",
    "absor_rolling2_divide_stride3 = divide_each_col(absor_rolling2, 5)\n",
    "absor_rolling2_divide_stride4 = divide_each_col(absor_rolling2, 10)\n",
    "\n",
    "absor_rolling3_divide_stride1 = divide_each_col(absor_rolling3, 1)\n",
    "absor_rolling3_divide_stride2 = divide_each_col(absor_rolling3, 3)\n",
    "absor_rolling3_divide_stride3 = divide_each_col(absor_rolling3, 5)\n",
    "absor_rolling3_divide_stride4 = divide_each_col(absor_rolling3, 10)\n",
    "\n",
    "absor_rolling4_divide_stride1 = divide_each_col(absor_rolling4, 1)\n",
    "absor_rolling4_divide_stride2 = divide_each_col(absor_rolling4, 3)\n",
    "absor_rolling4_divide_stride3 = divide_each_col(absor_rolling4, 5)\n",
    "absor_rolling4_divide_stride4 = divide_each_col(absor_rolling4, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########3\n",
    "\n",
    "\n",
    "\n",
    "absor_rho_rolling1_divide_stride1 = divide_each_col(absor_rho_rolling1, 1)\n",
    "absor_rho_rolling1_divide_stride2 = divide_each_col(absor_rho_rolling1, 3)\n",
    "absor_rho_rolling1_divide_stride3 = divide_each_col(absor_rho_rolling1, 5)\n",
    "absor_rho_rolling1_divide_stride4 = divide_each_col(absor_rho_rolling1, 10)\n",
    "\n",
    "absor_rho_rolling2_divide_stride1 = divide_each_col(absor_rho_rolling2, 1)\n",
    "absor_rho_rolling2_divide_stride2 = divide_each_col(absor_rho_rolling2, 3)\n",
    "absor_rho_rolling2_divide_stride3 = divide_each_col(absor_rho_rolling2, 5)\n",
    "absor_rho_rolling2_divide_stride4 = divide_each_col(absor_rho_rolling2, 10)\n",
    "\n",
    "absor_rho_rolling3_divide_stride1 = divide_each_col(absor_rho_rolling3, 1)\n",
    "absor_rho_rolling3_divide_stride2 = divide_each_col(absor_rho_rolling3, 3)\n",
    "absor_rho_rolling3_divide_stride3 = divide_each_col(absor_rho_rolling3, 5)\n",
    "absor_rho_rolling3_divide_stride4 = divide_each_col(absor_rho_rolling3, 10)\n",
    "\n",
    "absor_rho_rolling4_divide_stride1 = divide_each_col(absor_rho_rolling4, 1)\n",
    "absor_rho_rolling4_divide_stride2 = divide_each_col(absor_rho_rolling4, 3)\n",
    "absor_rho_rolling4_divide_stride3 = divide_each_col(absor_rho_rolling4, 5)\n",
    "absor_rho_rolling4_divide_stride4 = divide_each_col(absor_rho_rolling4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rho_rolling_1_divide_rolling_2 = divide_df(absor_rho_rolling1, absor_rho_rolling2)\n",
    "absor_rho_rolling_2_divide_rolling_3 = divide_df(absor_rho_rolling2, absor_rho_rolling3)\n",
    "absor_rho_rolling_3_divide_rolling_4 = divide_df(absor_rho_rolling3, absor_rho_rolling4)\n",
    "\n",
    "absor_rolling_1_divide_rolling_2 = divide_df(absor_rolling1, absor_rolling2)\n",
    "absor_rolling_2_divide_rolling_3 = divide_df(absor_rolling2, absor_rolling3)\n",
    "absor_rolling_3_divide_rolling_4 = divide_df(absor_rolling3, absor_rolling4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, absor_rolling1, on='id')\n",
    "df = pd.merge(df, absor_rolling2, on='id')\n",
    "df = pd.merge(df, absor_rolling3, on='id')\n",
    "df = pd.merge(df, absor_rolling4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4, on='id')\n",
    "\n",
    "##\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride4, on='id')\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling_1_divide_rolling_2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling_2_divide_rolling_3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling_3_divide_rolling_4, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.inf, np.nan)\n",
    "df = df.replace(-np.inf, np.nan)\n",
    "\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df):\n",
    "\n",
    "    X_train = df.drop(['hhb','hbo2','ca','na'], axis=1).loc[0:9999].copy()\n",
    "    X_test = df.drop(['hhb','hbo2','ca','na'], axis=1).loc[10000:20000].copy()\n",
    "\n",
    "    y_train = df[['hhb','hbo2','ca','na']].loc[0:9999].copy()\n",
    "    y_test = df[['hhb','hbo2','ca','na']].loc[10000:20000].copy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = get_train_test(df1)\n",
    "X_train2, X_test2, y_train2, y_test2 = get_train_test(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from eli5.permutation_importance import get_score_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.01,'random_state':42,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.5, 'metric':'l1'} #'\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "def my_permutation_badfeatures(X_train, y_train, col, kf):\n",
    "    y_train_temp = y_train[col].copy()\n",
    "    threshold = [0.001, 0.0001]\n",
    "    bad_features1 = []\n",
    "    bad_features2 = []\n",
    "    #bad_features3 = []\n",
    "        \n",
    "    def score(X, y):\n",
    "        y_pred = reg.predict(X)\n",
    "        return abs(y-y_pred).mean() \n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "        reg.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                      verbose=True, eval_metric='mae', early_stopping_rounds=50)\n",
    "        \n",
    "        \n",
    "        base_score, score_decreases = get_score_importances(score,np.array(val_x), np.array(val_y), n_iter=2)\n",
    "        \n",
    "        bad_features1.extend(list(val_x.columns[score_decreases[0] > -threshold[0]]))\n",
    "                             \n",
    "    return bad_features1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "##1\n",
    "for col in y_train1.columns:\n",
    "    \n",
    "    \n",
    "    bad_features1 = my_permutation_badfeatures(X_train1, y_train1, col, kf)\n",
    "    pd.DataFrame(bad_features1)[0].value_counts().to_csv(\"./bad_features/bad_features_gwang_\" + col + \"_1.csv\")\n",
    "\n",
    "##2\n",
    "for col in y_train2.columns:\n",
    "    \n",
    "    \n",
    "    bad_features1 = my_permutation_badfeatures(X_train2, y_train2, col, kf)\n",
    "    pd.DataFrame(bad_features1)[0].value_counts().to_csv(\"./bad_features/bad_features_gichan_\" + col + \"_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective':'reg:squarederror','eval_metric':'mae','colsample_bytree':0.7,'learning_rate':0.01,\n",
    "          'n_estimators':20000,'random_state':42, 'tree_method':'gpu_hist','n_gpus':1,'early_stopping_rounds':50}\n",
    "\n",
    "def my_permutation_lgb_for_stacking(X_train, X_test, y_train, y_test, param, kf, col):\n",
    "\n",
    "    score = []\n",
    "    \n",
    "    y_val_pred = y_train.copy()\n",
    "    y_val_pred.loc[:,:] = 0\n",
    "    \n",
    "    y_submit = y_test.copy()\n",
    "    y_submit.loc[:,:] = 0\n",
    "    \n",
    "    \n",
    "    valid_mae = []\n",
    "    \n",
    "    y_train_temp = y_train[col]\n",
    "    \n",
    "    \n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "\n",
    "    \n",
    "        reg.fit(trn_x, trn_y,\n",
    "                    eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                    early_stopping_rounds=50, verbose=100)\n",
    "    \n",
    "        pred = reg.predict(X_test)\n",
    "        y_submit[col] += pred/5\n",
    "        y_val_pred.loc[val_idx,col] = reg.predict(val_x)\n",
    "        \n",
    "        pred_val = reg.predict(val_x)\n",
    "        valid_mae.append(abs(pred_val-val_y).mean())\n",
    "        \n",
    "        \n",
    "    return y_submit[col], np.array(valid_mae).mean(), y_val_pred[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit_lgb1 = y_test1.copy()\n",
    "y_val_pred1 = y_train1.copy(); y_val_pred1.iloc[:,:] = 0\n",
    "\n",
    "y_submit_lgb2 = y_test2.copy()\n",
    "y_val_pred2 = y_train1.copy(); y_val_pred2.iloc[:,:] = 0\n",
    "\n",
    "## seed ensemble 실시한다 곧. random_stae = 777, 777^2, 777^4 \n",
    "param3 = {'objective':'regression','n_estimators':10000, 'learning_rate':0.15, 'random_state':777,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.2, 'metric':'l1','boosting':'dart'} #'boosting':'dart'\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "\n",
    "## 1\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "     \n",
    "    bad_features = pd.read_csv(\"./bad_features/bad_features_gwang_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features = bad_features[bad_features.iloc[:,0] == 5].index\n",
    "    List = list(bad_features)\n",
    "    \n",
    "    y_submit_lgb1[col], valid_mae, y_val_pred1[col] = my_permutation_lgb_for_stacking(X_train1.drop(List, axis=1), \\\n",
    "                                                      X_test1.drop(List, axis=1),y_train1,  y_test1, param3, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)\n",
    "\n",
    "## 3\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "     \n",
    "    bad_features = pd.read_csv(\"./bad_features/bad_features_gichan_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features = bad_features[bad_features.iloc[:,0] == 5].index\n",
    "    List = list(bad_features)\n",
    "    \n",
    "    y_submit_lgb2[col], valid_mae, y_val_pred2[col] = my_permutation_lgb_for_stacking(X_train2.drop(List, axis=1), \\\n",
    "                                                      X_test2.drop(List, axis=1),y_train2,  y_test2, param3, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stacking1 = y_val_pred1.append(y_submit_lgb1)\n",
    "y_stacking1.columns = y_stacking1.columns + \"_stacking\"\n",
    "\n",
    "y_stacking2 = y_val_pred2.append(y_submit_lgb2)\n",
    "y_stacking2.columns = y_stacking2.columns + \"_stacking\"\n",
    "\n",
    "#저장 \n",
    "y_stacking1.to_csv('./y_stacking1.csv')\n",
    "y_stacking2.to_csv('./y_stacking2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation + Stacking + dart + final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_all(y_stacking):\n",
    "    y_stacking_temp = y_stacking.copy()\n",
    "    for i in range(0, len(y_stacking.columns)-1):\n",
    "        for j in range(i+1, len(y_stacking.columns)):\n",
    "            y_stacking_temp[y_stacking.columns[i] + \"/\" + y_stacking.columns[j]] = y_stacking[y_stacking.columns[i]]/y_stacking[y_stacking.columns[j]] \n",
    "    return y_stacking_temp.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_permutation_lgb(X_train, X_test, y_train, y_test, param, kf, col):\n",
    "\n",
    "    score = []\n",
    "    \n",
    "    y_val_pred = y_train.copy()\n",
    "    y_val_pred.loc[:,:] = 0\n",
    "    \n",
    "    y_submit = y_test.copy()\n",
    "    y_submit.loc[:,:] = 0\n",
    "    \n",
    "    \n",
    "    valid_mae = []\n",
    "    \n",
    "    y_train_temp = y_train[col]\n",
    "    \n",
    "    \n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "\n",
    "    \n",
    "        reg.fit(trn_x, trn_y,\n",
    "                    eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                    early_stopping_rounds=50, verbose=100)\n",
    "    \n",
    "        pred = reg.predict(X_test)\n",
    "        y_submit[col] += pred/5\n",
    "        y_val_pred.loc[val_idx,col] = reg.predict(val_x)\n",
    "        \n",
    "        pred_val = reg.predict(val_x)\n",
    "        valid_mae.append(abs(pred_val-val_y).mean())\n",
    "        \n",
    "        \n",
    "    return y_submit[col], np.array(valid_mae).mean() , y_val_pred[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df1 stacking + df2 stacking -> dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['hhb','hbo2','ca','na'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stacking1= pd.read_csv('./y_stacking1.csv', index_col=0)\n",
    "y_stacking2 = pd.read_csv('./y_stacking2.csv', index_col=0)\n",
    "\n",
    "y_stacking1.index.name ='id'\n",
    "y_stacking2.index.name ='id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train_no, y_test = get_train_test(df_temp_final)\n",
    "y_submit_lgb_final = y_test1.copy()\n",
    "y_val_pred = y_train1.copy(); y_val_pred.iloc[:,:] = 0\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "#param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.005, 'random_state':777,\n",
    "#        'early_stopping_rounds':50,'colsample_bytree':0.7, 'metric':'l1','n_jobs':12}\n",
    "param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.15, 'random_state':777**2,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.2, 'metric':'l1','n_jobs':12,'boosting':'dart'} \n",
    "\n",
    "# seed : 777, 777*2, 777**2\n",
    "\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "    \n",
    "    Y0 = y_stacking1\n",
    "    Y1 = divide_all(y_stacking1)\n",
    "    \n",
    "    Y2 = y_stacking2\n",
    "    Y3 = divide_all(y_stacking2)    \n",
    "    \n",
    "    bad_features1 = pd.read_csv(\"./bad_features/bad_features_gwang_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features1 = bad_features1[bad_features1.iloc[:,0] == 5].index\n",
    "    List1 = list(bad_features1)\n",
    "    \n",
    "    bad_features2 = pd.read_csv(\"./bad_features/bad_features_gichan_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features2 = bad_features2[bad_features2.iloc[:,0] == 5].index\n",
    "    List2 = list(bad_features2)\n",
    "    \n",
    "    df_temp = df1.drop(List1, axis=1)\n",
    "    df_temp2 = df2.drop(List2, axis=1)\n",
    "    \n",
    "    # 합체!!\n",
    "    df_temp_final = pd.merge(df_temp, df_temp2, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y0, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y1, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y2, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y3, on='id')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_train_test(df_temp_final) \n",
    "    \n",
    "    y_submit_lgb_final[col], valid_mae, y_val_pred[col] = my_permutation_lgb(X_train,\n",
    "                                                      X_test,y_train,  y_test, param, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit_lgb_final.to_csv(\"./submission/submission_gichan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 광한 기찬 서브미션 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit_lgb_final = pd.read_csv('./submission/submission_gichan.csv')\n",
    "submission_gwang = pd.read_csv('./submission/0.77871_gwang_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = submission_gwang.copy()\n",
    "final[['hhb','hbo2','ca', 'na']] = (submission_gwang[['hhb','hbo2','ca', 'na']] + y_submit_lgb_final[['hhb','hbo2','ca', 'na']])/2\n",
    "final.to_csv('./submission/final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
